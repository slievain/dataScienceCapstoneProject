---
title: "Capstone Project - Milestone Report 1"
author: "SÃ©bastien Lievain"
date: "14/01/2017"
output:
  html_document:
    theme: united
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align='center')
```

## Objective
The objective of this capstone project is to put in practice all knowledge acquired during the previous 9 courses.  
The project will consist in showcasing in a shinyApp a predictive model of the next word someone is typing on a keyboard.

## Data retrieval and loading

Data must be downloaded from the following link:  
https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

```{r}
if(!file.exists("../Coursera-SwiftKey.zip")) {
    download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", 
                  "../Coursera-SwiftKey.zip", method = "curl")
    unzip("../Coursera-SwiftKey.zip")
}
```

The following files are available once the archive has been unzipped:

* final:
    + de_DE
    + en_US
    + fi_FI
    + ru_RU
    
Each language specific sub-folder contains three files:

* [**locale**].blogs.txt
* [**locale**].news.txt
* [**locale**].twitter.txt

In this project, we will focus on the en_US locale. Data can be loaded as follow:

```{r cache=TRUE}
US_twitter <- readLines("../final/en_US/en_US.twitter.txt", skipNul = TRUE)
US_blogs <- readLines("../final/en_US/en_US.blogs.txt", skipNul = TRUE)
US_news <- readLines("../final/en_US/en_US.news.txt", skipNul = TRUE)
```

The `skipNul` argument of the `readLines` function allows to skip NULL lines.

Some interesting facts on these files:
```{r echo=FALSE}
library(pander)

basic_stats <- data.frame(
    file_name = c("en_US.twitter.txt", "en_US.blogs.txt", "en_US.news.txt"),
    file_size = c(
        utils:::format.object_size(file.info("../final/en_US/en_US.twitter.txt")$size, "auto"),
        utils:::format.object_size(file.info("../final/en_US/en_US.blogs.txt")$size, "auto"),
        utils:::format.object_size(file.info("../final/en_US/en_US.news.txt")$size, "auto")
    ),
    nb_of_lines = c(
        format(length(US_twitter), decimal.mark=".", big.mark = ","),
        format(length(US_blogs), decimal.mark=".", big.mark = ","),
        format(length(US_news), decimal.mark=".", big.mark = ",")
    )
)
panderOptions('table.alignment.default', 'right')
pander(basic_stats)
```

## Exploratory analysis

### Subsetting strategy
With a total of `r format((length(US_twitter) + length(US_blogs) + length(US_news)), decimal.mark=".", big.mark = ",")` lines, the Corpus is way to big to be used entirely.

My strategy is to use 1% of each files:

```{r cache=TRUE}
sub_US_twitter <- US_twitter[as.logical(rbinom(length(US_twitter), 1, .01))]
sub_US_blogs <- US_blogs[as.logical(rbinom(length(US_blogs), 1, .01))]
sub_US_news <- US_news[as.logical(rbinom(length(US_news), 1, .01))]

sub_US <- c(sub_US_twitter, sub_US_blogs, sub_US_news)
```

```{r echo=FALSE}
rm(US_twitter)
rm(US_blogs)
rm(US_news)
rm(sub_US_twitter)
rm(sub_US_blogs)
rm(sub_US_news)
```

Using the `tm` package, I can create a Corpus that I will later use to apply data cleaning transformations and to create n-grams:
```{r}
library(tm)

corpus <- VCorpus(VectorSource(sub_US))
```

```{r echo=FALSE}
rm(sub_US)
```

### Data cleaning
I will apply several cleaning transformations on the data via the `td` package:
```{r echo=FALSE}
library(dplyr)
```

```{r cache=TRUE}
library(dplyr)

swearWords <- read.csv("../swearWords.csv", header = FALSE, stringsAsFactors = FALSE)

cleanCorpus <- tm_map(corpus, removeNumbers) %>%
    tm_map(content_transformer(function(x) gsub("(f|ht)tp(s?)://(.*)[.][a-z]+[^\\s.]+", " ", x, perl = TRUE))) %>%
    tm_map(content_transformer(function(x) gsub("[@#][^\\s]+", " ", x, perl = TRUE))) %>%
    tm_map(content_transformer(tolower)) %>%
    tm_map(content_transformer(function(x) iconv(x, "latin1", "ASCII", sub = ""))) %>%
    tm_map(removeWords, swearWords$V1) %>%
    tm_map(removePunctuation, preserve_intra_word_dashes = TRUE) %>%
    tm_map(stripWhitespace)
```

```{r echo=FALSE}
rm(corpus)
```

Other cleaning rules have been reviewed but not used:

* stemming words: as our application comprise a generation phase (prediction of the next word), it would have been necessary to add back what was removed from the stemming process which complexity was not worth the gain.
* foreign words: test of the `textcat` package showed very poor results. Therefore, I decided not to use it.

### Words distribution

We are now going to look at the distribution of words: uni-grams, bi-grams and tri-grams.  
Using the `tm` package, we can create a `DocumentTermMatrix` object which is a sparse matrix of Terms appearing in each Document.  
Using the `tidytext` package, we can manipulate the sparse matrix as a `data.frame`. 

We are first interested in the frequency of uni-grams (each Term in the Corpus):  
```{r echo=FALSE}
library(tidytext)
```

```{r cache=TRUE}
library(tidytext)

dtm_one <-DocumentTermMatrix(cleanCorpus)
    
dtm_one_td <- tidy(dtm_one) %>%
    group_by(term) %>%
    summarise(count = sum(count)) %>%
    arrange(desc(count))
```

Using the `wordcloud` package, we can represent uni-grams graphically:

```{r echo=FALSE}
library(wordcloud)

wordcloud(dtm_one_td$term, dtm_one_td$count, c(10,1), min.freq = 600, colors = brewer.pal(6, "Dark2"))
```

Then, we can calculate bi-grams using a special tokenizer during the creation of the sparse matrix:

```{r echo=FALSE}
options(java.parameters = "-Xmx14g")
options(mc.cores = 1)
library(RWeka)
```

```{r cache=TRUE}
options(java.parameters = "-Xmx14g")
options(mc.cores = 1)
library(RWeka)

BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))

dtm_two <- DocumentTermMatrix(cleanCorpus, control = list(tokenize = BigramTokenizer))
    
dtm_two_td <- tidy(dtm_two) %>% 
    group_by(term) %>%
    summarise(count = sum(count))

terms <- matrix(unlist(strsplit(dtm_two_td$term, " ")), ncol = 2, byrow = TRUE)

dtm_two_td <- dtm_two_td %>%
    mutate(
        first_word = terms[, 1],
        second_word = terms[, 2]) %>% 
    arrange(desc(count))
```

```{r echo=FALSE}
wordcloud(dtm_two_td$term, dtm_two_td$count, min.freq = 200, colors = brewer.pal(6, "Dark2"))
```

Finally, we can calculate tri-grams:

```{r cache=TRUE}
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

dtm_three <- DocumentTermMatrix(cleanCorpus, control = list(tokenize = TrigramTokenizer))
    
dtm_three_td <- tidy(dtm_three) %>% 
    group_by(term) %>%
    summarise(count = sum(count)) 

terms <- matrix(unlist(strsplit(dtm_three_td$term, " ")), ncol = 3, byrow = TRUE)

dtm_three_td <- dtm_three_td %>%
    mutate(
        first_two_words = paste(terms[, 1], terms[, 2]),
        third_word = terms[, 3]) %>% 
    arrange(desc(count))
```

```{r echo=FALSE}
wordcloud(dtm_three_td$term, dtm_three_td$count, min.freq = 40, colors = brewer.pal(6, "Dark2"))
```

We can look at the frequency of our n-grams using bar charts:

```{r fig.height=10, echo=FALSE}
library(ggplot2)
library(gridExtra)

plot1 <- ggplot(dtm_one_td[1:20, ], aes(reorder(term, desc(count)), count)) + 
    geom_bar(stat = "identity") + 
    ggtitle("Top 20 more recurrent unigrams") +
    labs(x = "Terms", y = "Count") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1), plot.title = element_text(hjust = 0.5))

plot2 <- ggplot(dtm_two_td[1:20, ], aes(reorder(term, desc(count)), count)) + 
    geom_bar(stat = "identity") + 
    ggtitle("Top 20 more recurrent bigrams") +
    labs(x = "Terms", y = "Count") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1), plot.title = element_text(hjust = 0.5))

plot3 <- ggplot(dtm_three_td[1:20, ], aes(reorder(term, desc(count)), count)) + 
    geom_bar(stat = "identity") + 
    ggtitle("Top 20 more recurrent trigrams") +
    labs(x = "Terms", y = "Count") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1), plot.title = element_text(hjust = 0.5))

grid.arrange(plot1, plot2, plot3)

```

The distribution of our n-grams is similar but the highest count decrease when N increases:

```{r fig.height=4, echo=FALSE}
p1 <- ggplot(dtm_one_td, aes(count))   + geom_density(fill = "grey35", col = "grey35") + ggtitle("Unigrams") + theme(plot.title = element_text(hjust = 0.5))
p2 <- ggplot(dtm_two_td, aes(count))   + geom_density(fill = "grey35", col = "grey35") + ggtitle("Bigrams") + theme(plot.title = element_text(hjust = 0.5))
p3 <- ggplot(dtm_three_td, aes(count)) + geom_density(fill = "grey35", col = "grey35") + ggtitle("Trigrams") + theme(plot.title = element_text(hjust = 0.5))

grid.arrange(p1, p2, p3, ncol = 3)
```

Here is a summary of the number of observed unique n-gram compared to the vocabulary size:

```{r echo=FALSE}
word_stats <- data.frame(
    unigram = c(
        format(length(unique(dtm_one_td$term)), decimal.mark=".", big.mark = ","),
        format(length(unique(dtm_one_td$term)), decimal.mark=".", big.mark = ","),
        paste0(format(100 * length(unique(dtm_one_td$term)) / length(unique(dtm_one_td$term)), decimal.mark=".", big.mark = ","), "%")
    ),
    bigram = c(
        format(length(unique(dtm_two_td$term)), decimal.mark=".", big.mark = ","),
        format(length(unique(dtm_one_td$term))^2, decimal.mark=".", big.mark = ","),
        paste0(format(100 * length(unique(dtm_two_td$term)) / length(unique(dtm_one_td$term))^2, decimal.mark=".", big.mark = ","), "%")
    ),
    trigram = c(
        format(length(unique(dtm_three_td$term)), decimal.mark=".", big.mark = ","),
        format(length(unique(dtm_one_td$term))^3, decimal.mark=".", big.mark = ","),
        paste0(format(100 * length(unique(dtm_three_td$term)) / length(unique(dtm_one_td$term))^3, decimal.mark=".", big.mark = ","), "%")
    )
)
row.names(word_stats) <- c("# of observed unique n-gram", "(V, V^2, V^3)", "pourcentage")
pander(word_stats)
```

V obviously corresponds to the Vocabulary size: The unique number of uni-grams taken into account.

## Coverage

One of the future objective will be to find the correct balance between accuracy and performance.  
It will therfore be necessary to truncate our dictionnary in order to increase performance and reduce memory footprint.

Let's calculate the number words we need, in a frequency sorted dictionary, to cover 50% of all word instances in the language? 90%?

```{r}
cumul_nb <- 0
i <- 1

while(cumul_nb < (.5 * sum(dtm_one_td$count))) {
    cumul_nb <- cumul_nb + dtm_one_td[i, ]$count
    i <- i + 1
}
```

The number of words required to cover 50% of all word instances is `r i`.  
The number of instances of the word at the 50% threshold is `r dtm_one_td[i - 1, ]$count`.
```{r}
while(cumul_nb < (.9 * sum(dtm_one_td$count))) {
    cumul_nb <- cumul_nb + dtm_one_td[i, ]$count
    i <- i + 1
}
```

The number of words required to cover 90% of all word instances is `r i`.  
The number of instances of the word at the 90% threshold is `r dtm_one_td[i - 1, ]$count`.

In the future, we will use this method to choose the minimum number of instances for a word to be kept in the dictionnary.

## Thoughts for a prediction model

I am going to create a predictive model based on a tri-gram language model which is based on the theory of second-order Markow Processes.

As seen above the percentage of observed tri-grams in our Corpus compared to the total number is very small: `r paste0(format(100 * length(unique(dtm_three_td$term)) / length(unique(dtm_one_td$term))^3, decimal.mark=".", big.mark = ","), "%")`. Meaning that any non-observed tri-gram in the training Corpus will never be proposed during the testing phase.  
Therefore, I am going to apply a discounting method: The Katz Back-Off Model.

In order to evaluate the accuracy of the produced model, I think about different strategies:

* A measure of perplexity which will be based on the probability of test sentences to be produced from our model
* A "real life" accuracy measure of 3 proposed following words: 3 words will be proposed to the user. If one of them match then we count a hit. At the end, we will count hits / total.

The corpus will require to be split into a training and testing sets.












