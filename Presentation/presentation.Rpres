Coursera Data Science Capstone Project
========================================================
author: SÃ©bastien Lievain
date: 16/02/2017
transition: rotate
autosize: true

The data product created during this Capstone Project aims at reproducing the well known Swiftkeys keyboard prediction feature.  

It is hosted on shinyapps.io and therefore the 1 Go constraint was taken into account when optimizing the accuracy / performance tradeoff.

<img style="float: left;" src="img/coursera.png">

<img style="float: right;" src="img/JHU.png">

<br clear="all" />

How it works
========================================================

The data product is organised with a traditionnal two-column layout.  
The left panel holds parameters the User can interact with:
* **Phrase**: It has to be exactly two-words long or an error message will be displayed.
* **Algorithm**: The User can choose between the quick "Stupid Back-off model" or the more accurate "Katz back-off model".
* **Show X entries**: It will impact the number of displayed results.

<center><img src="img/data_product.png"></center>

Prediction Models
========================================================
Our prediction model is based on a Tri-grams Language Model (2^nd order Markov property).

In order to deal with unobserved tri-grams, two different models have been implemented:
- Stupid back-off: 
    * It produces good results when applied on very large N-grams tables.
    * It doesn't account for unobserved tri-grams as such but instead backs off to lower n-grams. Therefore, it is very quick.
- Katz back-off:
    * In addition to a back-off "component", it also includes a discounting method (form of smoothing) in order to estimate probabilities of unobserved tri-grams.


The Performance vs Accuracy tradeoff
========================================================
<br />
## Accuracy

The accuracy of the model was first calculated using Perplexity.  
Unfortunately, the result was not stable when increasing the test dataset size and therefore not interpretable.

=> I finally decided to estimate the accuracy by measuring the number of hits (among top 3 predicted words) vs total number of tests.

## Performance

Performance was obtained by timing the execution of both the Stupid and Katz back-off algorithms.

Acceptable limit artificially set to 15 seconds.

***
<br />
## Model Parameters

Parameters of the model used to optimize the Performance vs Accuracy tradeoff were:

* Sampling of the training set (30% or 100%)
* Filtering or not bigrams and trigrams with only 1 occurence
* Filtering words from dictonnary based on the distribution of unigrams frequency sorted descending (80%, 85% and 90%)

```{r echo=FALSE, fig.width=12, fig.height=6, fig.align = 'center'}
library(tidyverse)
library(openxlsx)
library(ggplot2)
library(scales)
library(gridExtra)

as_percent <- function(strings) {
    sapply(strings, function(string) {
        num <- as.numeric(string)
        paste0(round(num * 100, 0), "%")
    })
}

results <- read.xlsx("../results.xlsx")

# First plot
tmp <- results %>%
    filter(filter_one_freq %in% c(FALSE)) %>%
    select(ratio, cutoff)

plot1 <- ggplot(results[results$filter_one_freq == TRUE, ], aes(x = factor(cutoff), y = stupidPred, fill = stupidPred <= 15)) +
    geom_bar(stat = "identity", width = 0.6, position = position_dodge(width = 0.7)) +
    facet_grid(~ ratio, labeller = labeller(ratio = as_percent)) +
    scale_x_discrete(
        breaks = c("0.8", "0.85", "0.9"),
        labels = c("80%", "85%", "90%")) +
    labs(x = "Cutoff", y = "Stupid Back-off timing (sec)", title = "Performance by Ratio and Cutoff values", subtitle = "N-grams with a frequency of 1 filtered") +
    theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) +
    geom_hline(yintercept = 15) +
    scale_fill_manual(name = 'Timing < 15s', values = setNames(c('forestgreen', 'red2'), c(T, F)), breaks = c(T, F), labels = c("True", "False"))

plot2 <- ggplot(results[results$filter_one_freq == TRUE, ], aes(x = factor(ratio), y = accuracy, fill = stupidPred <= 15)) + 
    geom_bar(stat = "identity", width = 0.6, position = position_dodge(width = 0.7)) + 
    facet_grid(~ cutoff, labeller = labeller(cutoff = as_percent)) + 
    scale_y_continuous(labels = percent) +
    scale_x_discrete(
        breaks = c("0.2", "0.25", "0.3", "1"),
        labels = c("20%", "25%", "30%", "100%")) +
    labs(x = "Ratio", y = "Accuracy", title = "Accuracy by Cutoff & Ratio values", subtitle = "N-grams with a frequency of 1 filtered") +
    theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) +
    scale_fill_manual(name = 'Timing < 15s', values = setNames(c('forestgreen', 'red2'), c(T, F)), breaks = c(T, F), labels = c("True", "False"))

grid.arrange(plot1, plot2, nrow = 1, ncol = 2)
```


Results and Resources
========================================================
The optimization task led to the selection of a ratio of 20% and a cutoff of 90% with bigrams and trigrams with a frequency of one filtered out.

Link to my data product on shinyapps.io:  
https://slievain.shinyapps.io/data_science-capstone_project/

The code of the data product, reports and scripts can be found on GitHub:  
https://github.com/slievain/...

Some videos from Michael Collins of Columbia who gave an excellent class on NLP:  
https://www.youtube.com/playlist?list=PLO9y7hOkmmSH7-p6que1MYbhBx74AzH7-  
https://www.youtube.com/playlist?list=PLO9y7hOkmmSHE2v_oEUjULGg20gyb-v1u

